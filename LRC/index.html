<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Long Range Compressor (LRC)</title>

<style>
    body{
        font-family: Sans-Serif;
        font-size:0.8em;
        color:#333333;
    }

    #outer{
        width:80%;
        margin: 0 auto;
    }
    
    table{
        border-spacing:0.8em;
        margin-left:1em;
    }
    
    td{
        vertical-align:top;   
    }
    
    th{
        text-align:left;   
    }
</style>
</head>
<body>
    <div id="outer">
        <h1>Long Range Compressor (LRC)</h1>

        <p>Updated 27/02/2006</p>

        <h2>Introduction</h2>

	<p>Since creating this page I have been told about <a href="http://rzip.samba.org/">rzip</a>, so this idea isn't new after all.</p>

        <p>This program attempts to detect and exploit duplication in streams
        which normal compression programs such as gzip and bzip2 fail to. The
        program uses a rolling checksum inspired by rsync to efficiently detect
        large blocks of data repeated arbitrarily far apart, distances much
        further than bzip2 notices. Imagine a tar / pax file of an application
        directory. The same dll may appear more than once in the file structure
        or the same library may have been statically linked to different
        executables. These duplicated blocks may be arbitrarily far apart in the
        archive.</p>

        <p>I will not attempt to explain in detail how the algorithm works here
        but feel free to download the source or email me. I will probably write
        up a description once the source has settled down.</p>
    
    <h2>Results</h2>
<h3>Absolute worst case for LRC</h3>

    <p>A file of any size containing no repeated blocks large enough for LRC to
detect will result in LRC adding 4 bytes to the length of the file. Here random1
is a file generated from /dev/urandom.</p>

    <table>
        <tr><th>File</th><th>Size (in bytes)</th></tr>
        <tr><td>random1</td><td>7,654,321</td></tr>
        <tr><td>random1.bz2</td><td>7,688,794</td></tr>
        <tr><td>random1.lrc</td><td>7,654,325</td></tr>
        <tr><td>random1.lrc.bz2</td><td>7,687,749</td></tr>
    </table>

    <h3>Absolute best case for LRC</h3>

    <p>A large block of random data several megabytes in size repeating itself is
ideal for LRC. The algorithm will spot the duplication of the entire block and
squash out all of the repetition. Here random2 is the above random1 file
repeated twice.</p>

    <table>
        <tr><th>File</th><th>Size (in bytes)</th></tr>
        <tr><td>random2</td><td>15,308,642</td></tr>
        <tr><td>random2.bz2</td><td>15,376,676</td></tr>
        <tr><td>random2.lrc</td><td>7,774,358</td></tr>
        <tr><td>random2.lrc.bz2</td><td>7,722,119</td></tr>
    </table>

    <p>LRC will demonstrate the same behaviour if the block is repeated many
    more times, ie only one copy of the block will be kept. This is true even if
    the copies of the block are spread throughout the input at large
    distances.</p>

    <h3>Some real data</h3>

    <p>Here some real filesystem trees have been tested. The LRC program does not
attempt to duplicate what bzip2 and others already do for local compression. For
that reason LRC output still benefits from being compressed itself with
bzip2.</p>

    <table>
        <tr>
            <th>File or directory</th>
            <th>Original (.pax or .tar)</th>
            <th>.bz2</th>
            <th>.lrc</th>
            <th>.lrc.bz2</th>
            <th>Comments</th>
       </tr>
       <tr>
            <td>/etc (approx 3100+ files)</td>
            <td>23,265,280</td>
            <td>2,985,250</td>
            <td>22,129,132</td>
            <td>2,961,793</td>
            <td>As expected not much saving here as /etc doesn't contain large portions of
repeated data.</td>
        </tr>
        <tr>
            <td>/usr/X11R6 (approx 3400+ files)</td>
            <td>148,746,240</td>
            <td>43,125,635</td>
            <td>86,368,716</td>
            <td>35,845,001</td>
            <td>Better compression here as presumably chunks of libraries / binaries duplicate
compiled object code.</td>
        </tr>
        <tr>
            <td>linux-2.6.14.7.tar (2.6.14.7 kernel source)</td>
            <td>224,102,400</td>
            <td>39,194,997</td>
            <td>214,558,252</td>
            <td>38,552,800</td>
            <td>Not very great compression here, again the files are mostly text and not binary.</td>
        </tr>
        <tr>
            <td>linux-2.6.14.7-2.6.15.4.tar (2.6.14.7 and 2.6.15.4 kernel source tar files concatenated)</td>
            <td>452,270,080</td>
            <td>79,018,004</td>
            <td>253,090,140</td>
            <td>44,601,962</td>
            <td>Here LRC is doing an excellent job of spotting the similarities between the two
archives despite the repetitions being over 200 megabytes apart in the stream.
This is a good example of how LRC works well when used with bzip2.</td>
        </tr>

    </table>

    <h2>Performance</h2>

    <h3>Compression</h3>

    <p>In terms of speed the current algorithm takes time O( N log( N ) ) where N is
the length of the input stream. This should be easy to improve to O( N ) by
replacing some tree maps with hash maps.</p>

    <p>In terms of memory used during compression LRC currently uses O( N ). This
looks bad but in reality the memory used is really a small fraction of the input
stream size. The above 400 megabyte files required only 40 megs of memory during
compression. The constant fraction can no doubt be improved but the O( N ) is
unavoidable.</p>

    <h3>Decompression</h3>

    <p>This is easy but requires seeking around in the output file as it is
reconstructed.</p>

    <h2>Issues</h2>

    <p>The current implementation is not optimized or very well tested. Some of the
tunable parameters have not been tested across different settings.</p>

    <h2>Download</h2>

    <p>This is very much work in progress, use at your own risk. The file format is
likely to change in the near future so do not archive anything important.</p>

    <p>Subversion: svn://pauldoo.dyndns.org/home/svn/Programs/LRC</p>


    <h2>Contact</h2>

    <p><a href="mailto:paul.richards@gmail.com">paul.richards@gmail.com</a></p>
  </div>
</body>
</html>

